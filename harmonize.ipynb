{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9af7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\spark-3.4.1-bin-hadoop3')\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"globant_test\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.6,com.microsoft.azure:azure-storage:8.6.6\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a62f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "account_key = config['azure']['accountKey']\n",
    "account_name = config['azure']['accountName']\n",
    "raw_container = config['azure']['rawContainer']\n",
    "silver_container = config['azure']['silverContainer']\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(f\"fs.azure.account.key.{account_name}.dfs.core.windows.net\", account_key)\n",
    "\n",
    "raw_fs = f'abfss://{raw_container}@{account_name}.dfs.core.windows.net'\n",
    "hmz_fs = f'abfss://{silver_container}@{account_name}.dfs.core.windows.net'\n",
    "file_path = 'globant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "603aa995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "hired_employees_schema = \\\n",
    "              StructType([ \\\n",
    "                        StructField('id', IntegerType(), True), \\\n",
    "                        StructField('name', StringType(), True), \\\n",
    "                        StructField('datetime', DateType(), True), \\\n",
    "                        StructField('department_id', IntegerType(), True), \\\n",
    "                        StructField('job_id', IntegerType(), True) \\\n",
    "                        ]) \n",
    "\n",
    "departments_schema = \\\n",
    "              StructType([ \\\n",
    "                        StructField('id', IntegerType(), True), \\\n",
    "                        StructField('department', StringType(), True) \\\n",
    "                        ])\n",
    "\n",
    "jobs_schema = StructType([ \\\n",
    "                        StructField('id', IntegerType(), True), \\\n",
    "                        StructField('job', StringType(), True) \\\n",
    "                        ])\n",
    "\n",
    "config = {'hired_employees': hired_employees_schema, 'departments': departments_schema, 'jobs': jobs_schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6978c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = {}\n",
    "for table, schema in config.items():\n",
    "    df_[table] = spark.read.csv(f'{raw_fs}/{file_path}/{table}.csv', header=False, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d64a97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta_ = {}\n",
    "for table in config:\n",
    "    if DeltaTable.isDeltaTable(spark, f'{hmz_fs}/{file_path}/{table}_delta'):\n",
    "        df_delta_[table] = DeltaTable.forPath(spark, f'{hmz_fs}/{file_path}/{table}_delta')\n",
    "        df_delta_[table].alias('current').merge( \\\n",
    "            df_[table].alias('new'), \\\n",
    "            condition=\"current.id  = new.id\" \\\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "        \n",
    "        df_delta_[table].optimize().executeCompaction()\n",
    "    else:\n",
    "        df_[table].write.format(\"delta\").mode(\"overwrite\").save(f'{hmz_fs}/{file_path}/{table}_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6f8970d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
